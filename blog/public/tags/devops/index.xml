<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>devops on /* Commentary */</title>
    <link>/tags/devops/</link>
    <description>Recent content in devops on /* Commentary */</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 13 Aug 2022 08:12:39 -0700</lastBuildDate><atom:link href="/tags/devops/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Who&#39;s the Boss?</title>
      <link>/posts/docker-swarm-managers/</link>
      <pubDate>Sat, 13 Aug 2022 08:12:39 -0700</pubDate>
      
      <guid>/posts/docker-swarm-managers/</guid>
      <description>Every node in a Docker Swarm plays at least one of two particular roles: worker and/or manager. When you create a new cluster, you start out by creating it with a single node that is both a manager and a worker, and specify an IP address to advertise that manager to other managers as you add them.
Roles So what are the responsibilities of these two roles anyway? How do you know how many managers and workers to deploy in your swarm?</description>
      <content>&lt;p&gt;Every node in a Docker Swarm plays at least one of two particular roles: worker
and/or manager. When you create a new cluster, you start out by creating it with
a single node that is both a manager and a worker, and &lt;a href=&#34;https://docs.docker.com/engine/swarm/admin_guide/#configure-the-manager-to-advertise-on-a-static-ip-address&#34;&gt;specify an IP address
to advertise that manager to other managers&lt;/a&gt; as you add them.&lt;/p&gt;
&lt;h1 id=&#34;roles&#34;&gt;Roles&lt;/h1&gt;
&lt;p&gt;So what are the responsibilities of these two roles anyway? How do you know
how many managers and workers to deploy in your swarm? Let&amp;rsquo;s get into it!&lt;/p&gt;
&lt;h2 id=&#34;managers&#34;&gt;Managers&lt;/h2&gt;
&lt;p&gt;As you might imagine, &lt;a href=&#34;https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/#manager-nodes&#34;&gt;manager nodes&lt;/a&gt; in a Docker Swarm take on
additional cluster management tasks. Such tasks include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;maintaining &amp;amp; distributing cluster state across manager nodes in the cluster&lt;/li&gt;
&lt;li&gt;scheduling services to various nodes in the swarm&lt;/li&gt;
&lt;li&gt;serving swarm mode HTTP API endpoints&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;workers&#34;&gt;Workers&lt;/h2&gt;
&lt;p&gt;Intuitively, &lt;a href=&#34;https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/#worker-nodes&#34;&gt;worker nodes&lt;/a&gt; don&amp;rsquo;t perform any of the management
tasks that the manager nodes do; all that stuff is above the pay grade.
Instead, worker nodes are solely responsible for executing service tasks.&lt;/p&gt;
&lt;h2 id=&#34;promotion--demotion&#34;&gt;Promotion &amp;amp; Demotion&lt;/h2&gt;
&lt;p&gt;Roles for nodes can be changed. To &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/node_promote/&#34;&gt;promote&lt;/a&gt; a worker node to
be a manager node, run the following:&lt;/p&gt;



  &lt;div class=&#34;collapsable-code&#34;&gt;
    &lt;input id=&#34;1&#34; type=&#34;checkbox&#34;  /&gt;
    &lt;label for=&#34;1&#34;&gt;
      &lt;span class=&#34;collapsable-code__language&#34;&gt;bash&lt;/span&gt;
      
      &lt;span class=&#34;collapsable-code__toggle&#34; data-label-expand=&#34;Show&#34; data-label-collapse=&#34;Hide&#34;&gt;&lt;/span&gt;
    &lt;/label&gt;
    &lt;pre class=&#34;language-bash&#34; &gt;&lt;code&gt;
$ docker node promote &amp;lt;NODE NAME&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;


&lt;p&gt;And to &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/node_demote/&#34;&gt;demote&lt;/a&gt; a node, run this command:&lt;/p&gt;



  &lt;div class=&#34;collapsable-code&#34;&gt;
    &lt;input id=&#34;2&#34; type=&#34;checkbox&#34;  /&gt;
    &lt;label for=&#34;2&#34;&gt;
      &lt;span class=&#34;collapsable-code__language&#34;&gt;bash&lt;/span&gt;
      
      &lt;span class=&#34;collapsable-code__toggle&#34; data-label-expand=&#34;Show&#34; data-label-collapse=&#34;Hide&#34;&gt;&lt;/span&gt;
    &lt;/label&gt;
    &lt;pre class=&#34;language-bash&#34; &gt;&lt;code&gt;
$ docker node demote &amp;lt;NODE NAME&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/em&gt;: By default, all manager nodes are also worker nodes. Assuming you
have a cluster that has more than one node, you can prevent any one manager node
from taking on worker node responsibilities by moving it&amp;rsquo;s availability status
to &lt;code&gt;Drain&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;deployment-strategies&#34;&gt;Deployment Strategies&lt;/h1&gt;
&lt;p&gt;Okay, so we&amp;rsquo;ve nailed down the roles that nodes can play within a swarm, but you
may be wondering:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How do I choose which role(s) to assign to a node?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It&amp;rsquo;s a great question, and one that can have pretty large impact.&lt;/p&gt;
&lt;h2 id=&#34;to-manage-or-not-to-manage&#34;&gt;To Manage or Not to Manage&lt;/h2&gt;
&lt;p&gt;Just like any business, having too many or too little managers can be problematic.
When it comes to provisioning managers in a Docker Swarm, you want to strike the
right balance of &lt;strong&gt;resiliency&lt;/strong&gt; and &lt;strong&gt;performance&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s touch first on resiliency.&lt;/p&gt;
&lt;h3 id=&#34;resiliency&#34;&gt;Resiliency&lt;/h3&gt;
&lt;p&gt;One of the primary reasons why we are using an orchestration engine like Docker
Swarm is so that we can have a degree of fault tolerance. If any particular
node in the swarm decides to take an &amp;ldquo;unnannounced vacation&amp;rdquo;, we should be able
to schedule whatever tasks (containers that comprise a service) to a different,
active node.&lt;/p&gt;
&lt;p&gt;We have to be particularly strategic to provide this fault tolerance when it comes
to managers. After all, they are responsible for taking on some pretty important
and necessary tasks. In order to best understand how many managers we should
deploy, we should quickly touch on the algorithm Docker Swarm uses to ensure
consistent state.&lt;/p&gt;
&lt;h4 id=&#34;raft-consensus-algorithm&#34;&gt;Raft Consensus Algorithm&lt;/h4&gt;
&lt;p&gt;Sounds fancy right?&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;http://thesecretlivesofdata.com/raft/&#34;&gt;Raft Consensus Algorithm&lt;/a&gt; is essentially an algorithm that
Docker Swarm uses to make sure each manager in the cluster is storing the same
consistent state. The most important thing to note in the context of this discussion
is that manager nodes need to agree on what the state is, and that &amp;ldquo;agreement&amp;rdquo;
is achieved as a function of how many nodes share the same information. If the
majority, also referred to as a &amp;ldquo;quorum&amp;rdquo;, is achieved when new state is proposed,
then it will be distributed to all managers.&lt;/p&gt;
&lt;p&gt;Approaching this mathematically, we can essentially use the following simple
equations to determine achievable fault tolerance and quorum:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;given:
  N = the number of manager nodes in the cluster.

then:
  number of manager nodes that can be down = (N-1)/2
  number of manager nodes to achieve quorum = (N/2)+1
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;EXAMPLE:&lt;/em&gt;&lt;/strong&gt; Assuming we have &lt;code&gt;3&lt;/code&gt; manager nodes (&lt;code&gt;N = 3&lt;/code&gt;) in a cluster of &lt;code&gt;5&lt;/code&gt;
total nodes, we would be able to support &lt;code&gt;1&lt;/code&gt; manager node being down at any
moment in time (&lt;code&gt;(3-1)/2 = 1&lt;/code&gt;). The number of manager nodes to reach quorum
would be &lt;code&gt;2&lt;/code&gt; (&lt;code&gt;(3/2)+1 = 2&lt;/code&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ultimately, the number of managers you deploy in your cluster depends somewhat
on the total number of nodes in your cluster. It is recommended you have at least
&lt;code&gt;3&lt;/code&gt; manager nodes, as such a configuration allows for one manager node to be down.&lt;/p&gt;
&lt;h3 id=&#34;performance&#34;&gt;Performance&lt;/h3&gt;
&lt;p&gt;As with much in software design, the resiliency comes at a cost. Since one element
of increasing the resiliency involves introducing redundency, more work is required
to spread information as we introduce more managers. Adding more manager nodes,
means more managers are needed to meet quorum.&lt;/p&gt;
&lt;p&gt;When it comes to Docker Swarm, it&amp;rsquo;s generally &lt;a href=&#34;https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/#manager-nodes&#34;&gt;not recommended to have any more
than &lt;code&gt;7&lt;/code&gt; manager nodes&lt;/a&gt;. Such a configuration would allow &lt;code&gt;3&lt;/code&gt;
managers to be simultaneously down at any moment and require &lt;code&gt;4&lt;/code&gt; nodes to reach
quorum.&lt;/p&gt;
&lt;h3 id=&#34;unreachable-quorum&#34;&gt;Unreachable Quorum&lt;/h3&gt;
&lt;p&gt;In the event that the number of manager nodes simultaneously down exceeds the
allowable fault tolerance, the state of the cluster is not allowed to change.
New tasks cannot be scheduled, and the existing tasks cannot be rebalanced to
other nodes if required. The following error will be returned when a management
operation is attempted:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Error response from daemon: rpc error: code = 4 desc = context deadline exceeded
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The ideal way to recover here is to get manager nodes back online. If for some
reason the managers can&amp;rsquo;t be brought back online, a new cluster will need to
be provisioned. This can be performed using the following command:&lt;/p&gt;



  &lt;div class=&#34;collapsable-code&#34;&gt;
    &lt;input id=&#34;1&#34; type=&#34;checkbox&#34;  /&gt;
    &lt;label for=&#34;1&#34;&gt;
      &lt;span class=&#34;collapsable-code__language&#34;&gt;bash&lt;/span&gt;
      
      &lt;span class=&#34;collapsable-code__toggle&#34; data-label-expand=&#34;Show&#34; data-label-collapse=&#34;Hide&#34;&gt;&lt;/span&gt;
    &lt;/label&gt;
    &lt;pre class=&#34;language-bash&#34; &gt;&lt;code&gt;
$ docker swarm init --force-new-cluster --advertise-addr &amp;lt;IP&amp;gt;:2377
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;


&lt;p&gt;Make sure to replace &lt;code&gt;&amp;lt;IP&amp;gt;&lt;/code&gt; with the IP address or DNS resolvable name of the
host you desire to be the initial manager of this new cluster.&lt;/p&gt;
&lt;h4 id=&#34;workers-get-shit-done&#34;&gt;Workers Get Shit Done&lt;/h4&gt;
&lt;p&gt;Compared to manager nodes, worker nodes are far simpler. Since they are
designated for executing the tasks for the swarm, the main deployment concerns
are simply ensuring that there are enough workers to balance the load.&lt;/p&gt;
&lt;p&gt;To join an existing swarm cluster as a worker node, run the following commands
to retrieve the worker join token:&lt;/p&gt;



  &lt;div class=&#34;collapsable-code&#34;&gt;
    &lt;input id=&#34;3&#34; type=&#34;checkbox&#34;  /&gt;
    &lt;label for=&#34;3&#34;&gt;
      &lt;span class=&#34;collapsable-code__language&#34;&gt;bash&lt;/span&gt;
      
      &lt;span class=&#34;collapsable-code__toggle&#34; data-label-expand=&#34;Show&#34; data-label-collapse=&#34;Hide&#34;&gt;&lt;/span&gt;
    &lt;/label&gt;
    &lt;pre class=&#34;language-bash&#34; &gt;&lt;code&gt;
$ docker swarm join-token worker
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;


&lt;p&gt;Assuming the above command provided this output:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker swarm join \
  --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
  192.168.99.100:2377
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The final step would be to run that command:&lt;/p&gt;



  &lt;div class=&#34;collapsable-code&#34;&gt;
    &lt;input id=&#34;4&#34; type=&#34;checkbox&#34;  /&gt;
    &lt;label for=&#34;4&#34;&gt;
      &lt;span class=&#34;collapsable-code__language&#34;&gt;bash&lt;/span&gt;
      
      &lt;span class=&#34;collapsable-code__toggle&#34; data-label-expand=&#34;Show&#34; data-label-collapse=&#34;Hide&#34;&gt;&lt;/span&gt;
    &lt;/label&gt;
    &lt;pre class=&#34;language-bash&#34; &gt;&lt;code&gt;
$ docker swarm join \
  --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \
  192.168.99.100:2377
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;NOTE&lt;/strong&gt;&lt;/em&gt;: When adding workers to accommodate for expected or current
workloads, make sure to take the various resource related requirements that
may be specified for a particular service. Remember that workers that
don&amp;rsquo;t meet those specified requirements will not be scheduled to execute
those tasks.&lt;/p&gt;
&lt;/blockquote&gt;
</content>
    </item>
    
    <item>
      <title>You Had One Job Portainer Console!</title>
      <link>/posts/portainer-console-issue/</link>
      <pubDate>Fri, 12 Aug 2022 22:04:10 -0700</pubDate>
      
      <guid>/posts/portainer-console-issue/</guid>
      <description>I use Portainer as my central hub for managing my Docker Swarm that powers my home lab, and it&amp;rsquo;s awesome.
One of the many features it includes is the ability to open up a &amp;lsquo;console&amp;rsquo; to any container (task) running in the swarm - think basically a UI for docker exec -it &amp;lt;CONTAINER ID&amp;gt; /bin/bash. It even provides the affordance of specifying a different user (other than the default of root) or shell (other than the default of bash).</description>
      <content>&lt;p&gt;I use &lt;a href=&#34;https://www.portainer.io/&#34;&gt;Portainer&lt;/a&gt; as my central hub for managing my Docker Swarm
that powers my home lab, and it&amp;rsquo;s awesome.&lt;/p&gt;
&lt;p&gt;One of the many features it includes is the ability to open up a &amp;lsquo;console&amp;rsquo; to any
container (task) running in the swarm - think basically a UI for
&lt;code&gt;docker exec -it &amp;lt;CONTAINER ID&amp;gt; /bin/bash&lt;/code&gt;. It even provides the affordance of
specifying a different user (other than the default of &lt;code&gt;root&lt;/code&gt;) or shell (other
than the default of &lt;code&gt;bash&lt;/code&gt;).&lt;/p&gt;
&lt;h1 id=&#34;connecting&#34;&gt;Connecting&amp;hellip;&lt;/h1&gt;
&lt;p&gt;One day, I went to hop into a console session on one of my containers, but instead
of the shell launching, Portainer seemed to be stuck when attempting to connect.&lt;/p&gt;

  &lt;figure class=&#34;center&#34; &gt;
    &lt;img src=&#34;/img/portainer-connecting-issue.png&#34;  alt=&#34;Portainer Connecting Issue&#34;   /&gt;
    
      &lt;figcaption class=&#34;center&#34; &gt;Screenshot of the issue.&lt;/figcaption&gt;
    
  &lt;/figure&gt;


&lt;p&gt;What gives? No error toasts appear in the top right to examine as one might expect.
Time to dig deeper.&lt;/p&gt;
&lt;p&gt;Once I landed in the logs, I tripped over some of these:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;...

level=info msg=&amp;#34;2022/08/13 05:06:35 websocketproxy: Error when copying from backend to client: websocket: close 1006 (abnormal closure): unexpected EOF&amp;#34;
level=info msg=&amp;#34;2022/08/13 05:07:01 websocketproxy: Error when copying from backend to client: websocket: invalid close code&amp;#34;

...

level=info msg=&amp;#34;2022/08/13 05:12:22 websocketproxy: couldn&amp;#39;t upgrade websocket: the client is not using the websocket protocol: &amp;#39;upgrade&amp;#39; token not found in &amp;#39;Connection&amp;#39; header&amp;#34;
level=info msg=&amp;#34;2022/08/13 05:12:40 websocketproxy: couldn&amp;#39;t upgrade websocket: the client is not using the websocket protocol: &amp;#39;upgrade&amp;#39; token not found in &amp;#39;Connection&amp;#39; header&amp;#34;

...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Interesting. Seems Portainer is attempting to make use of some websockets but can&amp;rsquo;t.&lt;/p&gt;
&lt;h1 id=&#34;root-cause--fix&#34;&gt;Root Cause + Fix&lt;/h1&gt;
&lt;p&gt;To take a half a step back real quick, it&amp;rsquo;s worth mentioning that I use nginx as
my reverse proxy of choice for my swarm. Essentially it runs as a container
within the swarm and forwards all traffic on ports &lt;code&gt;80&lt;/code&gt; (http) or &lt;code&gt;443&lt;/code&gt; (https)
to other services. One of these services is Portainer.&lt;/p&gt;
&lt;p&gt;As it turns out, my nginx configuration did not support websockets. I had to make
the following changes:&lt;/p&gt;



  &lt;div class=&#34;collapsable-code&#34;&gt;
    &lt;input id=&#34;1&#34; type=&#34;checkbox&#34;  /&gt;
    &lt;label for=&#34;1&#34;&gt;
      &lt;span class=&#34;collapsable-code__language&#34;&gt;diff&lt;/span&gt;
      
      &lt;span class=&#34;collapsable-code__toggle&#34; data-label-expand=&#34;Show&#34; data-label-collapse=&#34;Hide&#34;&gt;&lt;/span&gt;
    &lt;/label&gt;
    &lt;pre class=&#34;language-diff&#34; &gt;&lt;code&gt;
server {
	server_name ~^sweet.domain.com$;

	listen 443 ssl;
	ssl_certificate /ssl/cert.pem;
	ssl_certificate_key /ssl/key.pem;

	access_log /var/log/nginx/data-access.log combined;

	location / {
		proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
		proxy_pass https://sweet_swarm_task:9443;
&amp;#43;		proxy_http_version 1.1;
&amp;#43;		proxy_set_header Upgrade $http_upgrade;
&amp;#43;		proxy_set_header Connection &amp;#34;upgrade&amp;#34;;
	}
}
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;


&lt;p&gt;I definitely wasn&amp;rsquo;t the only one to make this mistake; this
&lt;a href=&#34;https://github.com/portainer/portainer/issues/6353&#34;&gt;GitHub issue&lt;/a&gt; being one example! Although, reading through
some of the conversation retriggered some interest in exploring
&lt;a href=&#34;https://nginxproxymanager.com/&#34;&gt;Nginx Proxy Manager&lt;/a&gt; once again - perhaps more on that
in a future post.&lt;/p&gt;
&lt;h4 id=&#34;breakdown&#34;&gt;Breakdown&lt;/h4&gt;
&lt;p&gt;In case you are saying:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hold up, what are these changes?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I got you covered. Let&amp;rsquo;s break it down.&lt;/p&gt;
&lt;h5 id=&#34;proxy_http_version-11&#34;&gt;&lt;code&gt;proxy_http_version 1.1;&lt;/code&gt;&lt;/h5&gt;
&lt;p&gt;Indicates that the HTTP version should be set to &lt;code&gt;1.1&lt;/code&gt;. Connection upgrades
can only be performed for &lt;code&gt;1.1&lt;/code&gt;, and not &lt;code&gt;1.0&lt;/code&gt; or &lt;code&gt;2&lt;/code&gt;
(&lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Protocol_upgrade_mechanism#upgrading_to_a_websocket_connection&#34;&gt;explicitly disallowed&lt;/a&gt;).&lt;/p&gt;
&lt;h5 id=&#34;proxy_set_header-upgrade-http_upgrade&#34;&gt;&lt;code&gt;proxy_set_header Upgrade $http_upgrade;&lt;/code&gt;&lt;/h5&gt;
&lt;p&gt;Sets the value of the &lt;code&gt;Upgrade&lt;/code&gt; &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Upgrade&#34;&gt;header&lt;/a&gt; to have the value
provided (see &lt;a href=&#34;%5Bhttps://nginx.org/en/docs/http/ngx_http_core_module.html#var_http_%5D&#34;&gt;&lt;code&gt;$http_*&lt;/code&gt; nginx variables&lt;/a&gt;). Clients utilize
this header to indicate that they wish to upgrade the connection from a
particular version of HTTP (1.1 for example) to another version, or to a websocket
connection.&lt;/p&gt;
&lt;h5 id=&#34;proxy_set_header-connection-upgrade&#34;&gt;&lt;code&gt;proxy_set_header Connection &amp;quot;upgrade&amp;quot;;&lt;/code&gt;&lt;/h5&gt;
&lt;p&gt;Sets the &lt;code&gt;Connection&lt;/code&gt; &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Connection&#34;&gt;header&lt;/a&gt; to have a value of
&lt;code&gt;&amp;quot;upgrade&amp;quot;&lt;/code&gt;, which indicates that nginx needs to process the &lt;code&gt;Upgrade&lt;/code&gt;
&lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Upgrade&#34;&gt;header&lt;/a&gt; before forwarding. This header must be sent whenever
the &lt;code&gt;Upgrade&lt;/code&gt; is sent.&lt;/p&gt;
&lt;h1 id=&#34;further-reading&#34;&gt;Further Reading&lt;/h1&gt;
&lt;p&gt;I&amp;rsquo;ve only scratched the surface on this stuff. If you are interesting in reading
further, check out &lt;a href=&#34;https://www.nginx.com/blog/websocket-nginx/&#34;&gt;this awesome post&lt;/a&gt; on the
&lt;a href=&#34;https://www.nginx.com/blog&#34;&gt;nginx blog&lt;/a&gt;.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Tale of the Unknown GitHub Secrets</title>
      <link>/posts/github-environments-and-secrets/</link>
      <pubDate>Wed, 10 Aug 2022 20:43:31 -0700</pubDate>
      
      <guid>/posts/github-environments-and-secrets/</guid>
      <description>When building out the deployment pipeline for this blog, I had my first run-in with Github Actions. In particular, my goal was to automatically generate a new Docker image when I push changes to the GitHub repository containing the source files, and push that Docker image to my corresponding DockerHub repository.
What secrets? In order to achieve this, I reached for docker/login-action. This straightforward action essentially takes the credentials provided to it and logs into DockerHub using those credentials.</description>
      <content>&lt;p&gt;When building out the deployment pipeline for this blog, I had my first run-in
with &lt;a href=&#34;https://github.com/features/actions&#34;&gt;Github Actions&lt;/a&gt;. In particular, my goal was to automatically generate a
new Docker image when I push changes to the GitHub repository containing
the source files, and push that Docker image to my corresponding DockerHub repository.&lt;/p&gt;
&lt;h1 id=&#34;what-secrets&#34;&gt;What secrets?&lt;/h1&gt;
&lt;p&gt;In order to achieve this, I reached for &lt;a href=&#34;https://github.com/marketplace/actions/docker-login&#34;&gt;&lt;code&gt;docker/login-action&lt;/code&gt;&lt;/a&gt;. This straightforward
action essentially takes the credentials provided to it and logs into DockerHub
using those credentials.&lt;/p&gt;
&lt;p&gt;I went ahead and bootstrapped a &lt;a href=&#34;https://github.com/fr33r/.com-blog/blob/main/.github/workflows/docker.yml&#34;&gt;workflow configuration&lt;/a&gt; based on the example
provided in the &lt;a href=&#34;https://github.com/marketplace/actions/docker-login&#34;&gt;documentation for the DockerHub registry&lt;/a&gt;, and created the
&lt;code&gt;DOCKERHUB_USERNAME&lt;/code&gt; and &lt;code&gt;DOCKERHUB_PASSWORD&lt;/code&gt; secrets in GitHub.&lt;/p&gt;
&lt;p&gt;Then it was time to test. I went ahead and pushed some changes to trigger the
workflow, and then&amp;hellip;&lt;/p&gt;



  &lt;div class=&#34;collapsable-code&#34;&gt;
    &lt;input id=&#34;1&#34; type=&#34;checkbox&#34;  /&gt;
    &lt;label for=&#34;1&#34;&gt;
      &lt;span class=&#34;collapsable-code__language&#34;&gt;output&lt;/span&gt;
      
      &lt;span class=&#34;collapsable-code__toggle&#34; data-label-expand=&#34;Show&#34; data-label-collapse=&#34;Hide&#34;&gt;&lt;/span&gt;
    &lt;/label&gt;
    &lt;pre class=&#34;language-output&#34; &gt;&lt;code&gt;
Run docker/login-action@v2
  with:
    ecr: auto
    logout: true
Error: Username and password required
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;


&lt;p&gt;Hmm, strange.&lt;/p&gt;
&lt;p&gt;I know I created the secrets and &lt;a href=&#34;https://github.com/fr33r/.com-blog/blob/352aa1c2bc88345e05efe3f9174643f0789e2abf/.github/workflows/docker.yml#L25-L26&#34;&gt;specified them in the workflow configuration&lt;/a&gt;.
Why didn&amp;rsquo;t they come through? Is this a generic error message, or does it truly
get displayed when the credentials are missing? Did I forget to add values for the
secrets? Do the keys for the secrets line up in the workflow configuration?&lt;/p&gt;
&lt;p&gt;To help me find some answers, I reran the failed workflow with debug logs enabled:&lt;/p&gt;



  &lt;div class=&#34;collapsable-code&#34;&gt;
    &lt;input id=&#34;2&#34; type=&#34;checkbox&#34;  /&gt;
    &lt;label for=&#34;2&#34;&gt;
      &lt;span class=&#34;collapsable-code__language&#34;&gt;output&lt;/span&gt;
      
      &lt;span class=&#34;collapsable-code__toggle&#34; data-label-expand=&#34;Show&#34; data-label-collapse=&#34;Hide&#34;&gt;&lt;/span&gt;
    &lt;/label&gt;
    &lt;pre class=&#34;language-output&#34; &gt;&lt;code&gt;
##[debug]Evaluating condition for step: &amp;#39;Login to DockerHub&amp;#39;
##[debug]Evaluating: success()
##[debug]Evaluating success:
##[debug]=&amp;gt; true
##[debug]Result: true
##[debug]Starting: Login to DockerHub
##[debug]Register post job cleanup for action: docker/login-action@v2
##[debug]Loading inputs
##[debug]Evaluating: secrets.DOCKERHUB_USERNAME
##[debug]Evaluating Index:
##[debug]..Evaluating secrets:
##[debug]..=&amp;gt; Object
##[debug]..Evaluating String:
##[debug]..=&amp;gt; &amp;#39;DOCKERHUB_USERNAME&amp;#39;
##[debug]=&amp;gt; null
##[debug]Result: null
##[debug]Evaluating: secrets.DOCKERHUB_TOKEN
##[debug]Evaluating Index:
##[debug]..Evaluating secrets:
##[debug]..=&amp;gt; Object
##[debug]..Evaluating String:
##[debug]..=&amp;gt; &amp;#39;DOCKERHUB_TOKEN&amp;#39;
##[debug]=&amp;gt; null
##[debug]Result: null
##[debug]Loading env
Run docker/login-action@v2
  with:
    ecr: auto
    logout: true
::save-state name=isPost::true
##[debug]Save intra-action state isPost = true
::save-state name=registry::
##[debug]Save intra-action state registry = 
::save-state name=logout::true
##[debug]Save intra-action state logout = true
Error: Username and password required
##[debug]Node Action run completed with exit code 1
##[debug]Finishing: Login to DockerHub
&lt;/code&gt;&lt;/pre&gt;
  &lt;/div&gt;


&lt;p&gt;Okay. So the workfow is attempting to resolve the secrets, but it seems it is printing
out &lt;code&gt;null&lt;/code&gt; for both the &lt;code&gt;DOCKERHUB_USERNAME&lt;/code&gt; and &lt;code&gt;DOCKERHUB_TOKEN&lt;/code&gt;. I remember seeing
&lt;a href=&#34;https://docs.github.com/en/actions/security-guides/encrypted-secrets#accessing-your-secrets&#34;&gt;something&lt;/a&gt; about values of secrets being redacted from action output, but does that mean
that it will print &lt;code&gt;null&lt;/code&gt;? That can&amp;rsquo;t be right&amp;hellip;&lt;/p&gt;
&lt;h1 id=&#34;root-cause--fix&#34;&gt;Root Cause + Fix&lt;/h1&gt;
&lt;p&gt;After spending far too much time turning over these stones, I finally discovered the
concept I was lacking knowledge on: GitHub environments.&lt;/p&gt;
&lt;p&gt;Being totally new to setting up secrets in GitHub actions, I had missed the &lt;a href=&#34;https://docs.github.com/en/actions/deployment/targeting-different-environments/using-environments-for-deployment#using-an-environment&#34;&gt;documentation&lt;/a&gt;
that described how workflows interacted with GitHub environments. Equipped with this ignorance,
I proceeded to create the GitHub secrets as environment secrets, not repository
secrets. Combine this naiveness with the documentation provided for
&lt;code&gt;docker/login-action&lt;/code&gt;, which outlines examples that implicitly assumes
repository level secrets are used, and the end result is that the workflow
configuration I created couldn&amp;rsquo;t access the secrets I had created.&lt;/p&gt;
&lt;p&gt;There were two solutions for here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;make my workflow configuration environment aware, or&lt;/li&gt;
&lt;li&gt;remove the existing secrets I had created and add them as repository level secrets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given I had created the environment without much intention, and because I currently
don&amp;rsquo;t have much of a use case for supporting multiple environments for this project,
I chose to go with the latter approach. Once the credentials were moved (and the old
ones removed), it worked like a charm!&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Had I chosen to go with the first option, I would&amp;rsquo;ve added an &lt;code&gt;environment&lt;/code&gt;
key within the workflow configuration, as outlined in &lt;a href=&#34;https://docs.github.com/en/actions/deployment/targeting-different-environments/using-environments-for-deployment#using-an-environment&#34;&gt;this&lt;/a&gt; GitHub documentation.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h1 id=&#34;how-could-this-be-better&#34;&gt;How could this be better?&lt;/h1&gt;
&lt;p&gt;It&amp;rsquo;s clear that the root cause was my user error here. However, I definitely
think it is worth highlighting a couple of opportunities for improvement that
could prevent others from having to climb out of the same trap.&lt;/p&gt;
&lt;h4 id=&#34;better-error-messaging&#34;&gt;Better Error Messaging&lt;/h4&gt;
&lt;p&gt;Error messages could always be better right? When it comes to improving the feedback
loop, the first thing I identify is whether or not the application generating the error
message has enough information to actually generate a more descriptive message and/or
a suggested fix.&lt;/p&gt;
&lt;p&gt;In this situation, that is totally the case. It was clear here that the secrets being
referenced did in fact exist, but just in an environment and not at the repository level.
GitHub even has &lt;a href=&#34;https://docs.github.com/en/actions/security-guides/encrypted-secrets#naming-your-secrets&#34;&gt;documentation&lt;/a&gt; on how two secrets with the same key but exist in both levels
are treated, so awareness of secrets in both places is already being handled to some
degree.&lt;/p&gt;
&lt;p&gt;To illustrate, imagine the debug logs had entries like this instead:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;...

##[debug]..Evaluating secrets:
##[debug]..=&amp;gt; Object
##[debug]..Evaluating String:
##[debug]..=&amp;gt; &amp;#39;DOCKERHUB_TOKEN&amp;#39;
##[debug]..Checking repository secrets
##[debug]=&amp;gt; null
##[debug]..Checking environment: staging
##[debug]=&amp;gt; null
##[debug]..Checking environment: production
##[debug]...Match found. Did you mean to reference an environment secret instead?

...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With this verbiage, there is no need to dive in for triaging. Users would
know immediately why their secrets were not accessible.&lt;/p&gt;
&lt;p&gt;In a similar vein, it&amp;rsquo;s worth asking the question whether referencing secrets
when there are 0 secrets detected is ever an expected scenario. In this circumstance,
the workflow couldn&amp;rsquo;t find any secrets, despite both of them being listed. Perhaps
an error message indicating the following would be beneficial:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;...

##[debug]..Evaluating secrets:
##[debug]..=&amp;gt; Object
##[debug]..No secrets exist

...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This message would at least be more straightforward that no secrets are detected,
eliminating the additional steps the user would need to take to understand why
&lt;code&gt;null&lt;/code&gt; is being passed around.&lt;/p&gt;
&lt;h4 id=&#34;comprehensive-documentation&#34;&gt;Comprehensive Documentation&lt;/h4&gt;
&lt;p&gt;This one may be more of a reach, but it&amp;rsquo;s worth calling out that the documentation
for the &lt;code&gt;docker/login-action&lt;/code&gt; assumes that environments aren&amp;rsquo;t being used. I don&amp;rsquo;t
fault the maintainers for thinking it wouldn&amp;rsquo;t be necessary to address explicitly -
after all, GitHub environments is it&amp;rsquo;s own wide-spread feature, and it shouldn&amp;rsquo;t
be necessary for every maintainer of a GitHub action to eductate users of other
GitHub features.&lt;/p&gt;
&lt;p&gt;However, the maintainers &lt;em&gt;are&lt;/em&gt; responsible for writing the code to generate
error messages for their action. Their documentation doesn&amp;rsquo;t outline the potential
error that can be thrown, or potential troubleshooting steps.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
